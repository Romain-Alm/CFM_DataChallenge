{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.2\n"
     ]
    }
   ],
   "source": [
    "### importation des librairies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, GRU, Dropout\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid\n",
    "from tensorflow.keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from keras.layers import RepeatVector\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Dropout, BatchNormalization, Bidirectional, Flatten, concatenate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, GRU, Bidirectional, concatenate, Dropout, BatchNormalization, Embedding, MultiHeadAttention, GlobalAveragePooling1D, add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "print(tf.__version__)\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data_Shaping import *\n",
    "from models_check import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obs_id</th>\n",
       "      <th>venue</th>\n",
       "      <th>order_id</th>\n",
       "      <th>action</th>\n",
       "      <th>side</th>\n",
       "      <th>price</th>\n",
       "      <th>bid</th>\n",
       "      <th>ask</th>\n",
       "      <th>bid_size</th>\n",
       "      <th>ask_size</th>\n",
       "      <th>trade</th>\n",
       "      <th>flux</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>D</td>\n",
       "      <td>B</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>False</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     obs_id  venue  order_id action side  price  bid   ask  bid_size  \\\n",
       "0         0      4         0      A    A   0.30  0.0  0.01       100   \n",
       "1         0      4         1      A    B  -0.17  0.0  0.01       100   \n",
       "2         0      4         2      D    A   0.28  0.0  0.01       100   \n",
       "3         0      4         3      A    A   0.30  0.0  0.01       100   \n",
       "4         0      4         4      D    A   0.37  0.0  0.01       100   \n",
       "..      ...    ...       ...    ...  ...    ...  ...   ...       ...   \n",
       "115       1      4        11      A    B  -0.06  0.0  0.04         2   \n",
       "116       1      4        12      A    B  -0.01  0.0  0.04         2   \n",
       "117       1      4        13      A    A   0.06  0.0  0.04         2   \n",
       "118       1      2        14      A    B  -0.01  0.0  0.04         2   \n",
       "119       1      5        15      D    B  -0.78  0.0  0.04         2   \n",
       "\n",
       "     ask_size  trade  flux  \n",
       "0           1  False   100  \n",
       "1           1  False   100  \n",
       "2           1  False  -100  \n",
       "3           1  False   100  \n",
       "4           1  False  -100  \n",
       "..        ...    ...   ...  \n",
       "115        75  False   100  \n",
       "116        75  False   100  \n",
       "117        75  False     4  \n",
       "118        75  False     1  \n",
       "119        75  False  -100  \n",
       "\n",
       "[120 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### importation des données en pandas\n",
    "x_train_df = pd.read_csv(\"/Users/josealmeida/Desktop/Data Challenge/données/X_train_N1UvY30.csv\")\n",
    "y_train_df = pd.read_csv(\"/Users/josealmeida/Desktop/Data Challenge/données/y_train_or6m3Ta.csv\")\n",
    "x_test_df=pd.read_csv(\"/Users/josealmeida/Desktop/Data Challenge/données/X_test_m4HAPAP.csv\")\n",
    "x_train_df.head(120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split_data(x_train_df,y_train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_array, order_ids, venues, actions, sides, trades=prepare_data_pipeline(x_min)\n",
    "print(numeric_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_array, order_ids, venues, actions, sides, trades=prepare_data_pipeline(x_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_gru_model(input_shape=(100, 11), order_id_dim=100, action_dim=3, \n",
    "                      side_dim=2, venue_dim=6, trade_dim=2, learning_rate=3e-3):\n",
    "    \n",
    "    # Entrées principales pour les features numériques\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Couches d'embedding avec régularisation L2\n",
    "    order_id_input = Input(shape=(100,))\n",
    "    order_id_embedding = Embedding(input_dim=order_id_dim, output_dim=16, \n",
    "                                 embeddings_regularizer=l2(1e-4))(order_id_input)\n",
    "    \n",
    "    action_input = Input(shape=(100,))\n",
    "    action_embedding = Embedding(input_dim=action_dim, output_dim=8,\n",
    "                               embeddings_regularizer=l2(1e-4))(action_input)\n",
    "    \n",
    "    side_input = Input(shape=(100,))\n",
    "    side_embedding = Embedding(input_dim=side_dim, output_dim=4,\n",
    "                             embeddings_regularizer=l2(1e-4))(side_input)\n",
    "    \n",
    "    venue_input = Input(shape=(100,))\n",
    "    venue_embedding = Embedding(input_dim=venue_dim, output_dim=8,\n",
    "                              embeddings_regularizer=l2(1e-4))(venue_input)\n",
    "    \n",
    "    trade_input = Input(shape=(100,))\n",
    "    trade_embedding = Embedding(input_dim=trade_dim, output_dim=4,\n",
    "                              embeddings_regularizer=l2(1e-4))(trade_input)\n",
    "\n",
    "    # Concaténation des embeddings\n",
    "    concatenated_embeddings = concatenate([\n",
    "        order_id_embedding, action_embedding, side_embedding, \n",
    "        venue_embedding, trade_embedding\n",
    "    ], axis=-1)\n",
    "    \n",
    "    # Fusion des données numériques et catégorielles\n",
    "    x = concatenate([inputs, concatenated_embeddings], axis=-1)\n",
    "    \n",
    "    # Première couche de traitement avec skip connection\n",
    "    residual = x\n",
    "    x = Dense(256, activation='selu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = add([x, Dense(x.shape[-1])(residual)])  # Skip connection\n",
    "    \n",
    "    # Couches GRU bidirectionnelles avec skip connections\n",
    "    residual = x\n",
    "    x = Bidirectional(GRU(128, return_sequences=True,\n",
    "                         kernel_regularizer=l2(1e-4),\n",
    "                         recurrent_regularizer=l2(1e-4)))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = add([x, Dense(x.shape[-1])(residual)])  # Skip connection\n",
    "    \n",
    "    # Mécanisme d'attention multi-têtes\n",
    "    x = MultiHeadAttention(num_heads=4, key_dim=32)(x, x, x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Couches denses finales avec connexions résiduelles\n",
    "    residual = x\n",
    "    x = Dense(128, activation='selu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = add([x, Dense(128)(residual)])\n",
    "    \n",
    "    # Global average pooling pour réduire la dimension temporelle\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Couche de sortie avec régularisation\n",
    "    outputs = Dense(24, activation='softmax',\n",
    "                   kernel_regularizer=l2(1e-4))(x)\n",
    "    \n",
    "    # Construction et compilation du modèle\n",
    "    model = Model(\n",
    "        inputs=[inputs, order_id_input, action_input, \n",
    "                side_input, venue_input, trade_input],\n",
    "        outputs=outputs\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_gru_model2(input_shape=(100, 11), order_id_dim=100, venue_dim=6, \n",
    "                      action_dim=3, side_dim=2, trade_dim=2, learning_rate=3e-3):\n",
    "    \"\"\"\n",
    "    Modèle GRU avec attention et embeddings\n",
    "    L'ordre des entrées correspond exactement à prepare_data_pipeline\n",
    "    \"\"\"\n",
    "    # Entrées dans le même ordre que prepare_data_pipeline\n",
    "    numeric_input = Input(shape=input_shape, name='numeric_input')  # Pour numeric_array\n",
    "    order_id_input = Input(shape=(100,), name='order_id_input')    # Pour order_ids\n",
    "    venue_input = Input(shape=(100,), name='venue_input')          # Pour venues\n",
    "    action_input = Input(shape=(100,), name='action_input')        # Pour actions\n",
    "    side_input = Input(shape=(100,), name='side_input')           # Pour sides\n",
    "    trade_input = Input(shape=(100,), name='trade_input')         # Pour trades\n",
    "    \n",
    "    # Embeddings\n",
    "    order_embed = Embedding(input_dim=order_id_dim, output_dim=16, \n",
    "                          embeddings_regularizer=l2(1e-4))(order_id_input)\n",
    "    venue_embed = Embedding(input_dim=venue_dim, output_dim=8, \n",
    "                          embeddings_regularizer=l2(1e-4))(venue_input)\n",
    "    action_embed = Embedding(input_dim=action_dim, output_dim=4, \n",
    "                           embeddings_regularizer=l2(1e-4))(action_input)\n",
    "    side_embed = Embedding(input_dim=side_dim, output_dim=4, \n",
    "                          embeddings_regularizer=l2(1e-4))(side_input)\n",
    "    trade_embed = Embedding(input_dim=trade_dim, output_dim=4, \n",
    "                          embeddings_regularizer=l2(1e-4))(trade_input)\n",
    "    \n",
    "    # Concaténation\n",
    "    concat_embeddings = concatenate([\n",
    "        order_embed, venue_embed, action_embed, side_embed, trade_embed\n",
    "    ])\n",
    "    x = concatenate([numeric_input, concat_embeddings], axis=-1)\n",
    "    \n",
    "    # Dense layers avec skip connections\n",
    "    residual = x\n",
    "    x = Dense(256, activation='selu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = add([x, Dense(x.shape[-1])(residual)])\n",
    "    \n",
    "    # GRU layers\n",
    "    residual = x\n",
    "    x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = add([x, Dense(x.shape[-1])(residual)])\n",
    "    \n",
    "    # Attention\n",
    "    x = MultiHeadAttention(num_heads=4, key_dim=32)(x, x, x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Global pooling et sortie\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(128, activation='selu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    outputs = Dense(24, activation='softmax')(x)\n",
    "    \n",
    "    # Construction du modèle avec les entrées dans le même ordre que prepare_data_pipeline\n",
    "    model = Model(\n",
    "        inputs=[numeric_input, order_id_input, venue_input, \n",
    "                action_input, side_input, trade_input],\n",
    "        outputs=outputs\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes disponibles avant encodage: ['obs_id', 'venue', 'order_id', 'action', 'side', 'price', 'bid', 'ask', 'bid_size', 'ask_size', 'trade', 'flux', 'spread', 'order_imbalance', 'cumul_volume', 'price_change', 'price_anomaly']\n",
      "Colonnes disponibles après encodage: ['obs_id', 'venue', 'order_id', 'price', 'bid', 'ask', 'bid_size', 'ask_size', 'flux', 'spread', 'order_imbalance', 'cumul_volume', 'price_change', 'price_anomaly', 'action_encoded', 'side_encoded', 'trade_encoded']\n",
      "successfull normalizayion\n",
      "Colonnes disponibles avant encodage: ['obs_id', 'venue', 'order_id', 'action', 'side', 'price', 'bid', 'ask', 'bid_size', 'ask_size', 'trade', 'flux', 'spread', 'order_imbalance', 'cumul_volume', 'price_change', 'price_anomaly']\n",
      "Colonnes disponibles après encodage: ['obs_id', 'venue', 'order_id', 'price', 'bid', 'ask', 'bid_size', 'ask_size', 'flux', 'spread', 'order_imbalance', 'cumul_volume', 'price_change', 'price_anomaly', 'action_encoded', 'side_encoded', 'trade_encoded']\n",
      "successfull normalizayion\n",
      "Dimensions des données d'entrée:\n",
      "train_numeric shape: (128640, 100, 11)\n",
      "train_order unique values: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "train_venue unique values: [0 1 2 3 4 5]\n",
      "train_action unique values: [0 1 2]\n",
      "train_side unique values: [0 1]\n",
      "train_trade unique values: [0 1]\n",
      "\n",
      "Ranges des valeurs dans les données :\n",
      "train_order min: 0, max: 99\n",
      "train_venue min: 0, max: 5\n",
      "train_action min: 0, max: 2\n",
      "train_side min: 0, max: 1\n",
      "train_trade min: 0, max: 1\n",
      "\n",
      "Vérification des valeurs hors limites :\n",
      "train_order values > 99: 0\n",
      "train_venue values > 5: 0\n",
      "train_action values > 2: 0\n",
      "train_side values > 1: 0\n",
      "train_trade values > 1: 0\n",
      "Epoch 1/20\n",
      "\u001b[1m4020/4020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1346s\u001b[0m 334ms/step - accuracy: 0.1064 - loss: 2.8612 - val_accuracy: 0.1363 - val_loss: 2.7524 - learning_rate: 0.0030\n",
      "Epoch 2/20\n",
      "\u001b[1m4020/4020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1308s\u001b[0m 325ms/step - accuracy: 0.1486 - loss: 2.6828 - val_accuracy: 0.1095 - val_loss: 3.3072 - learning_rate: 0.0030\n",
      "Epoch 3/20\n",
      "\u001b[1m4020/4020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1326s\u001b[0m 330ms/step - accuracy: 0.1564 - loss: 2.5964 - val_accuracy: 0.1524 - val_loss: 2.7353 - learning_rate: 0.0030\n",
      "Epoch 4/20\n",
      "\u001b[1m4020/4020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1305s\u001b[0m 325ms/step - accuracy: 0.1702 - loss: 2.5696 - val_accuracy: 0.1433 - val_loss: 2.8370 - learning_rate: 0.0030\n",
      "Epoch 5/20\n",
      "\u001b[1m4020/4020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1493s\u001b[0m 371ms/step - accuracy: 0.1525 - loss: 2.6676 - val_accuracy: 0.1175 - val_loss: 2.9096 - learning_rate: 0.0030\n",
      "Epoch 6/20\n",
      "\u001b[1m4020/4020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1550s\u001b[0m 386ms/step - accuracy: 0.1448 - loss: 2.6802 - val_accuracy: 0.1361 - val_loss: 2.8297 - learning_rate: 0.0030\n",
      "Epoch 7/20\n",
      "\u001b[1m4020/4020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1264s\u001b[0m 314ms/step - accuracy: 0.1637 - loss: 2.6105 - val_accuracy: 0.1558 - val_loss: 2.8962 - learning_rate: 0.0015\n",
      "Epoch 8/20\n",
      "\u001b[1m4020/4020\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1259s\u001b[0m 313ms/step - accuracy: 0.1530 - loss: 2.6540 - val_accuracy: 0.1567 - val_loss: 2.8919 - learning_rate: 0.0015\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model28,history28= train_model(\n",
    "    model_function=lambda: enhanced_gru_model2(\n",
    "        input_shape=(100, 11),\n",
    "        order_id_dim=100,  \n",
    "        venue_dim=6,      \n",
    "        action_dim=3,     \n",
    "        side_dim=2,       \n",
    "        trade_dim=2,      \n",
    "        learning_rate=3e-3\n",
    "    ),\n",
    "    X=x_train_df,\n",
    "    Y=y_train_df,\n",
    "    epochs=20,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_gru_model(input_shape=(100, 11), order_id_dim=100, venue_dim=6, \n",
    "                   action_dim=3, side_dim=2, trade_dim=2, learning_rate=3e-3):\n",
    "    \"\"\"\n",
    "    Modèle GRU simplifié avec embeddings de base\n",
    "    \"\"\"\n",
    "    # Entrées numériques\n",
    "    numeric_input = Input(shape=input_shape)\n",
    "    \n",
    "    # Embeddings simples\n",
    "    order_id_input = Input(shape=(100,))\n",
    "    order_id_embedding = Embedding(input_dim=order_id_dim, output_dim=8)(order_id_input)\n",
    "    \n",
    "    action_input = Input(shape=(100,))\n",
    "    action_embedding = Embedding(input_dim=action_dim, output_dim=4)(action_input)\n",
    "    \n",
    "    side_input = Input(shape=(100,))\n",
    "    side_embedding = Embedding(input_dim=side_dim, output_dim=2)(side_input)\n",
    "    \n",
    "    venue_input = Input(shape=(100,))\n",
    "    venue_embedding = Embedding(input_dim=venue_dim, output_dim=3)(venue_input)\n",
    "    \n",
    "    trade_input = Input(shape=(100,))\n",
    "    trade_embedding = Embedding(input_dim=trade_dim, output_dim=2)(trade_input)\n",
    "\n",
    "    # Concaténation\n",
    "    concatenated_embeddings = concatenate([\n",
    "        order_id_embedding, action_embedding, side_embedding, \n",
    "        venue_embedding, trade_embedding\n",
    "    ], axis=-1)\n",
    "    \n",
    "    # Fusion avec données numériques\n",
    "    merged = concatenate([numeric_input, concatenated_embeddings], axis=-1)\n",
    "    \n",
    "    # Couches GRU simples\n",
    "    x = Bidirectional(GRU(64, return_sequences=True))(merged)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Bidirectional(GRU(32))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Couches denses finales\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    outputs = Dense(24, activation='softmax')(x)\n",
    "    \n",
    "    # Modèle\n",
    "    model = Model(\n",
    "        inputs=[numeric_input, order_id_input, venue_input,\n",
    "                action_input, side_input, trade_input],\n",
    "        outputs=outputs\n",
    "    )\n",
    "    \n",
    "    # Compilation\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m(\n\u001b[1;32m      2\u001b[0m     model_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m: basic_gru_model(),\n\u001b[1;32m      3\u001b[0m     X\u001b[38;5;241m=\u001b[39mx_train_df,\n\u001b[1;32m      4\u001b[0m     Y\u001b[38;5;241m=\u001b[39my_train_df,\n\u001b[1;32m      5\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m      6\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_model' is not defined"
     ]
    }
   ],
   "source": [
    "model, history = train_model(\n",
    "    model_function=lambda: basic_gru_model(),\n",
    "    X=x_train_df,\n",
    "    Y=y_train_df,\n",
    "    epochs=50,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_final_df=process_sequences(x_train_df)\n",
    "print(x_train_final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_rnn(df):\n",
    "    action_encoded=df['action'].map({'A': 0, 'D': 1, 'U': 2})\n",
    "    venue_encoded=df['venue']\n",
    "    side_encoded=df['side'].map({'A': 0, 'B': 1})\n",
    "    trade_encoded=df['trade'].map({True: 1, False: 0})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Définir les colonnes au niveau global\n",
    "NUMERIC_COLS = ['price', 'bid', 'ask', 'bid_size', 'ask_size', 'flux',\n",
    "               'spread', 'order_imbalance', 'cumul_volume', \n",
    "               'price_change', 'price_anomaly']\n",
    "\n",
    "CATEGORICAL_COLS = ['action', 'side', 'venue', 'trade', 'order_id']\n",
    "\n",
    "def prepare_data_for_rnn(df):\n",
    "    # 1. Encodage des variables catégorielles\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for col in CATEGORICAL_COLS:\n",
    "        le = LabelEncoder()\n",
    "        df[f'{col}_encoded'] = le.fit_transform(df[col])\n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    # 3. Reshape des données en séquences de 100\n",
    "    n_sequences = len(df) // 100\n",
    "    \n",
    "    # Préparer les inputs pour les embeddings\n",
    "    categorical_inputs = {\n",
    "        col: df[f'{col}_encoded'].values.reshape(n_sequences, 100)\n",
    "        for col in CATEGORICAL_COLS\n",
    "    }\n",
    "    \n",
    "    # Préparer les features numériques\n",
    "    numeric_features = df[NUMERIC_COLS].values.reshape(n_sequences, 100, len(NUMERIC_COLS))\n",
    "    \n",
    "    return numeric_features, categorical_inputs, label_encoders\n",
    "\n",
    "def train_rnn_model(x_train_df, y_train, input_shape):\n",
    "    # Préparer les données\n",
    "    numeric_features, categorical_inputs, label_encoders = prepare_data_for_rnn(x_train_df)\n",
    "    \n",
    "    # Obtenir les dimensions pour les embeddings\n",
    "    embedding_dims = {\n",
    "        'order_id': len(label_encoders['order_id'].classes_),\n",
    "        'action': len(label_encoders['action'].classes_),\n",
    "        'side': len(label_encoders['side'].classes_),\n",
    "        'venue': len(label_encoders['venue'].classes_),\n",
    "        'trade': len(label_encoders['trade'].classes_)\n",
    "    }\n",
    "    \n",
    "    # Créer et compiler le modèle\n",
    "    model = complexed_gru_reshaped2(\n",
    "        input_shape=(100, len(NUMERIC_COLS)),\n",
    "        order_id_dim=embedding_dims['order_id'],\n",
    "        action_dim=embedding_dims['action'],\n",
    "        side_dim=embedding_dims['side'],\n",
    "        venue_dim=embedding_dims['venue'],\n",
    "        trade_dim=embedding_dims['trade']\n",
    "    )\n",
    "    \n",
    "    # Préparer les inputs pour l'entraînement\n",
    "    model_inputs = [\n",
    "        numeric_features,\n",
    "        categorical_inputs['order_id'],\n",
    "        categorical_inputs['action'],\n",
    "        categorical_inputs['side'],\n",
    "        categorical_inputs['venue'],\n",
    "        categorical_inputs['trade']\n",
    "    ]\n",
    "    \n",
    "    # S'assurer que y_train a la bonne forme\n",
    "    n_sequences = len(x_train_df) // 100\n",
    "    y_train = y_train[:n_sequences]  # Prendre une étiquette par séquence\n",
    "    y_train_cat = to_categorical(y_train, num_classes=24)\n",
    "    \n",
    "    # Répéter les étiquettes pour chaque timestep si nécessaire\n",
    "    y_train_cat = np.repeat(y_train_cat[:, np.newaxis, :], 100, axis=1)\n",
    "    \n",
    "    # Entraîner le modèle\n",
    "    history = model.fit(\n",
    "        model_inputs,\n",
    "        y_train_cat,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "    \n",
    "    return model, history, label_encoders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "input_shape = (100, len(NUMERIC_COLS))  # Utilisation de la constante globale\n",
    "# Assurez-vous de n'utiliser que la colonne eqt_code_cat\n",
    "y_train = y_train_df['eqt_code_cat'].values  # Sélectionner uniquement la colonne de labels\n",
    "\n",
    "model, history, encoders = train_rnn_model(x_train_final_df, y_train, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valeurs uniques dans y_train:\", np.unique(y_train_df))\n",
    "print(\"\\nMin:\", y_train_df.min())\n",
    "print(\"Max:\", y_train_df.max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environnementML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
